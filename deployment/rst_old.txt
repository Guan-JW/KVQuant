args.seqlen=2048, args.maxseqlen=1024, args.include_sparse=True
splitting into 1 GPUs
Load quantizers.
k:  model.layers.0.self_attn.k_proj
k:  model.layers.0.self_attn.v_proj
k:  model.layers.1.self_attn.k_proj
k:  model.layers.1.self_attn.v_proj
k:  model.layers.2.self_attn.k_proj
k:  model.layers.2.self_attn.v_proj
k:  model.layers.3.self_attn.k_proj
k:  model.layers.3.self_attn.v_proj
k:  model.layers.4.self_attn.k_proj
k:  model.layers.4.self_attn.v_proj
k:  model.layers.5.self_attn.k_proj
k:  model.layers.5.self_attn.v_proj
k:  model.layers.6.self_attn.k_proj
k:  model.layers.6.self_attn.v_proj
k:  model.layers.7.self_attn.k_proj
k:  model.layers.7.self_attn.v_proj
k:  model.layers.8.self_attn.k_proj
k:  model.layers.8.self_attn.v_proj
k:  model.layers.9.self_attn.k_proj
k:  model.layers.9.self_attn.v_proj
k:  model.layers.10.self_attn.k_proj
k:  model.layers.10.self_attn.v_proj
k:  model.layers.11.self_attn.k_proj
k:  model.layers.11.self_attn.v_proj
k:  model.layers.12.self_attn.k_proj
k:  model.layers.12.self_attn.v_proj
k:  model.layers.13.self_attn.k_proj
k:  model.layers.13.self_attn.v_proj
k:  model.layers.14.self_attn.k_proj
k:  model.layers.14.self_attn.v_proj
k:  model.layers.15.self_attn.k_proj
k:  model.layers.15.self_attn.v_proj
k:  model.layers.16.self_attn.k_proj
k:  model.layers.16.self_attn.v_proj
k:  model.layers.17.self_attn.k_proj
k:  model.layers.17.self_attn.v_proj
k:  model.layers.18.self_attn.k_proj
k:  model.layers.18.self_attn.v_proj
k:  model.layers.19.self_attn.k_proj
k:  model.layers.19.self_attn.v_proj
k:  model.layers.20.self_attn.k_proj
k:  model.layers.20.self_attn.v_proj
k:  model.layers.21.self_attn.k_proj
k:  model.layers.21.self_attn.v_proj
Model type : llama
Benchmarking ...
0 0.3069765567779541
1 0.04249429702758789
2 0.041494131088256836
3 0.041579484939575195
4 0.04155588150024414
5 0.04172253608703613
6 0.04152536392211914
7 0.041626930236816406
8 0.04158449172973633
9 0.04169750213623047
10 0.04160022735595703
11 0.04152178764343262
12 0.04160284996032715
13 0.04167771339416504
14 0.04210615158081055
15 0.042313337326049805
16 0.041754722595214844
17 0.04160881042480469
18 0.04146075248718262
19 0.04145622253417969
20 0.04143166542053223
21 0.04137563705444336
22 0.04160261154174805
23 0.04135394096374512
24 0.04154562950134277
25 0.04161691665649414
26 0.041593313217163086
27 0.04146313667297363
28 0.04149436950683594
29 0.04153943061828613
30 0.041441917419433594
31 0.0414576530456543
32 0.04151177406311035
33 0.041321516036987305
34 0.04146981239318848
35 0.041399240493774414
36 0.041445016860961914
37 0.04144477844238281
38 0.041344642639160156
39 0.041815996170043945
40 0.04182243347167969
41 0.04182147979736328
42 0.04136037826538086
43 0.041289329528808594
44 0.04134368896484375
45 0.041283369064331055
46 0.04149198532104492
47 0.04146146774291992
48 0.04142403602600098
49 0.04134368896484375
50 0.041463375091552734
51 0.041611433029174805
52 0.04132819175720215
53 0.04140496253967285
54 0.04146242141723633
55 0.04176974296569824
56 0.04146075248718262
57 0.04120659828186035
58 0.0414891242980957
59 0.041535139083862305
60 0.04151272773742676
61 0.04156756401062012
62 0.041304826736450195
63 0.04133415222167969
64 0.0426177978515625
65 0.04154181480407715
66 0.041303157806396484
67 0.041422367095947266
68 0.04130053520202637
69 0.04142904281616211
70 0.041469573974609375
71 0.041327714920043945
72 0.0417935848236084
73 0.04137444496154785
74 0.0413355827331543
75 0.04143071174621582
76 0.04135608673095703
77 0.04139208793640137
78 0.04129910469055176
79 0.041277408599853516
80 0.04156637191772461
81 0.041445255279541016
82 0.041567087173461914
83 0.04132223129272461
84 0.041494131088256836
85 0.04198575019836426
86 0.04132962226867676
87 0.041495561599731445
88 0.04141044616699219
89 0.04158830642700195
90 0.0417485237121582
91 0.04142284393310547
92 0.04151320457458496
93 0.041287899017333984
94 0.041454315185546875
95 0.041324615478515625
96 0.041344404220581055
97 0.04117107391357422
98 0.04131054878234863
99 0.04152393341064453
100 0.041391611099243164
101 0.0415339469909668
102 0.041518211364746094
103 0.04142022132873535
104 0.04157114028930664
105 0.041266679763793945
106 0.042107582092285156
107 0.04122567176818848
108 0.04079747200012207
109 0.04085659980773926
110 0.04078841209411621
111 0.04074668884277344
112 0.040515899658203125
113 0.04073953628540039
114 0.040808677673339844
115 0.04053854942321777
116 0.040688276290893555
117 0.04055428504943848
118 0.04059171676635742
119 0.04060626029968262
120 0.040999650955200195
121 0.04086446762084961
122 0.04090738296508789
123 0.040860652923583984
124 0.04073524475097656
125 0.04084634780883789
126 0.040926218032836914
127 0.04121732711791992
Median: 0.04144489765167236
PPL: nan
max memory(MiB): 2123.24169921875
