args.seqlen=2048, args.maxseqlen=1024, args.include_sparse=True
splitting into 1 GPUs
Load quantizers.
k:  model.layers.0.self_attn.k_proj
k:  model.layers.0.self_attn.v_proj
k:  model.layers.1.self_attn.k_proj
k:  model.layers.1.self_attn.v_proj
k:  model.layers.2.self_attn.k_proj
k:  model.layers.2.self_attn.v_proj
k:  model.layers.3.self_attn.k_proj
k:  model.layers.3.self_attn.v_proj
k:  model.layers.4.self_attn.k_proj
k:  model.layers.4.self_attn.v_proj
k:  model.layers.5.self_attn.k_proj
k:  model.layers.5.self_attn.v_proj
k:  model.layers.6.self_attn.k_proj
k:  model.layers.6.self_attn.v_proj
k:  model.layers.7.self_attn.k_proj
k:  model.layers.7.self_attn.v_proj
k:  model.layers.8.self_attn.k_proj
k:  model.layers.8.self_attn.v_proj
k:  model.layers.9.self_attn.k_proj
k:  model.layers.9.self_attn.v_proj
k:  model.layers.10.self_attn.k_proj
k:  model.layers.10.self_attn.v_proj
k:  model.layers.11.self_attn.k_proj
k:  model.layers.11.self_attn.v_proj
k:  model.layers.12.self_attn.k_proj
k:  model.layers.12.self_attn.v_proj
k:  model.layers.13.self_attn.k_proj
k:  model.layers.13.self_attn.v_proj
k:  model.layers.14.self_attn.k_proj
k:  model.layers.14.self_attn.v_proj
k:  model.layers.15.self_attn.k_proj
k:  model.layers.15.self_attn.v_proj
k:  model.layers.16.self_attn.k_proj
k:  model.layers.16.self_attn.v_proj
k:  model.layers.17.self_attn.k_proj
k:  model.layers.17.self_attn.v_proj
k:  model.layers.18.self_attn.k_proj
k:  model.layers.18.self_attn.v_proj
k:  model.layers.19.self_attn.k_proj
k:  model.layers.19.self_attn.v_proj
k:  model.layers.20.self_attn.k_proj
k:  model.layers.20.self_attn.v_proj
k:  model.layers.21.self_attn.k_proj
k:  model.layers.21.self_attn.v_proj
Model type : llama
Benchmarking ...
0 0.3060743808746338
1 0.05839109420776367
2 0.05793905258178711
3 0.05744028091430664
4 0.057546138763427734
5 0.05833697319030762
6 0.05820727348327637
7 0.05838799476623535
8 0.059256553649902344
9 0.0582423210144043
10 0.05831289291381836
11 0.059129953384399414
12 0.05870628356933594
13 0.05841541290283203
14 0.05948209762573242
15 0.05937957763671875
16 0.0600132942199707
17 0.06045389175415039
18 0.058893442153930664
19 0.05898332595825195
20 0.059738874435424805
21 0.05886960029602051
22 0.059825897216796875
23 0.05872988700866699
24 0.05898714065551758
25 0.06009554862976074
26 0.05893826484680176
27 0.05912590026855469
28 0.060005903244018555
29 0.0595545768737793
30 0.05921220779418945
31 0.06011652946472168
32 0.059320926666259766
33 0.060483694076538086
34 0.06022191047668457
35 0.05941414833068848
36 0.059453725814819336
37 0.060051918029785156
38 0.05940985679626465
39 0.05959200859069824
40 0.06078052520751953
41 0.060153961181640625
42 0.06037616729736328
43 0.06100916862487793
44 0.06050252914428711
45 0.060914039611816406
46 0.06154322624206543
47 0.05986785888671875
48 0.05950522422790527
49 0.06067347526550293
50 0.05950212478637695
51 0.05971860885620117
52 0.0605473518371582
53 0.05957341194152832
54 0.06074714660644531
55 0.059455156326293945
56 0.059888601303100586
57 0.06146121025085449
58 0.06052446365356445
59 0.06052398681640625
60 0.061482906341552734
61 0.06066322326660156
62 0.0610659122467041
63 0.0614776611328125
64 0.061943769454956055
65 0.06086564064025879
66 0.06212115287780762
67 0.0608067512512207
68 0.060595035552978516
69 0.061434268951416016
70 0.0607149600982666
71 0.06058359146118164
72 0.06158137321472168
73 0.06085944175720215
74 0.06036496162414551
75 0.061066627502441406
76 0.06037425994873047
77 0.060376644134521484
78 0.06160998344421387
79 0.06093096733093262
80 0.0603792667388916
81 0.06160926818847656
82 0.06079459190368652
83 0.06160378456115723
84 0.06175708770751953
85 0.060765743255615234
86 0.06180596351623535
87 0.061004638671875
88 0.0609586238861084
89 0.06296515464782715
90 0.06072258949279785
91 0.06079268455505371
92 0.06257128715515137
93 0.06086540222167969
94 0.06076693534851074
95 0.06232643127441406
96 0.06135964393615723
97 0.06212806701660156
98 0.06347012519836426
99 0.06186985969543457
100 0.06161832809448242
101 0.0629875659942627
102 0.06167936325073242
103 0.061606645584106445
104 0.06274843215942383
105 0.06152915954589844
106 0.062113285064697266
107 0.06285524368286133
108 0.06157827377319336
109 0.06225752830505371
110 0.06290268898010254
111 0.061821937561035156
112 0.06194353103637695
113 0.0629267692565918
114 0.061811208724975586
115 0.061928749084472656
116 0.06260991096496582
117 0.06157255172729492
118 0.06276392936706543
119 0.06137990951538086
120 0.06168675422668457
121 0.06279945373535156
122 0.0613095760345459
123 0.061593055725097656
124 0.06287503242492676
125 0.061434268951416016
126 0.06180620193481445
127 0.06319117546081543
Median: 0.06076633930206299
PPL: nan
max memory(MiB): 2120.49169921875
