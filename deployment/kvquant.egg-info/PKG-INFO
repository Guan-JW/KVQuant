Metadata-Version: 2.1
Name: kvquant
Version: 0.1.0
Summary: KV Cache Quantization.
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: accelerate
Requires-Dist: sentencepiece
Requires-Dist: torch
Requires-Dist: datasets

# KVQuant Deployment Code

The code in this folder can be used to run the inference experiments from the paper (for benchmarking kernel runtime) and to run end-to-end generation with the compressed KV cache.

## Installation

1. Create a conda environment
```
conda create --name deploy python=3.9 -y
conda activate deploy
```

2. Clone and install the dependencies (including the local transformers environment)
```
cd deployment/transformers
pip install -e .
pip install -r requirements.txt
pip install accelerate -U
cd ..
pip install -e .
cd kvquant
python setup_cuda.py install
cd ..
<!-- pip install flash-attn==2.5.5 --no-build-isolation  -->
<!-- 安装 flash-attn 会报错 -->
<!-- RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback): -->
<!-- /home/storage20T/guanjiawei/anaconda3/envs/deploy/lib/python3.9/site-packages/flash_attn_2_cuda.cpython-39-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi -->

pip install protobuf
```

3. Run kernel benchmarking

Note that the quantizer is obtained from steps in the quant directory.

```
cp ../quant/quantizers.pickle .
CUDA_VISIBLE_DEVICES=0 python cache-llama-activations.py <path-to-llama-7b-hf> --wbits 4 --nsamples 1 --seqlen 2048 --quantizer-path quantizers.pickle --output-path activations.pickle;
```

```
CUDA_VISIBLE_DEVICES=0 python cache-llama-activations.py /home/storage20T/guanjw_backup/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6 --wbits 4 --nsamples 1 --seqlen 1024 --quantizer-path quantizers.pickle --output-path activations.pickle;
```

Assuming the activations and the quantizers are stored in "activations.pickle" and "quantizers.pickle", you can run the kernel benchmarks by running the python scripts in the "scripts" folder.

4. Run end-to-end generation

```
cp ../quant/quantizers.pickle .
CUDA_VISIBLE_DEVICES=0 python llama.py <path-to-llama-7b-hf> wikitext2 --abits 4 --include_sparse --sparsity-threshold 0.99 --quantizer-path quantizers.pickle --benchmark 128 --check
```

```
cp ../quant/quantizers.pickle .
CUDA_VISIBLE_DEVICES=0 python llama.py /home/storage20T/guanjw_backup/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6 wikitext2 --abits 4 --include_sparse --sparsity-threshold 0.99 --quantizer-path quantizers.pickle --benchmark 128 --check
```
